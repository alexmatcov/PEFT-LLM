{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d029f624",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500d161d",
   "metadata": {},
   "source": [
    "Install unsloth on colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07e4692",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip install python-dotenv\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git@nightly git+https://github.com/unslothai/unsloth-zoo.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ff4c6e",
   "metadata": {},
   "source": [
    "Get .env contents and keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d54ad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def load_hf_config():\n",
    "    \"\"\"\n",
    "    Load Hugging Face credentials from:\n",
    "      1. Colab userdata secrets (if running on Colab)\n",
    "      2. Local .env file (if running locally)\n",
    "      3. Existing environment variables (fallback)\n",
    "    \"\"\"\n",
    "    # --- Check if running in Colab ---\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        # If this works, we are in Colab.\n",
    "        username = userdata.get(\"HF_USERNAME\")\n",
    "        token = userdata.get(\"HF_TOKEN\")\n",
    "        repo_name = userdata.get(\"HF_REPO_NAME\") or \"PEFT-optimization\"\n",
    "\n",
    "        if username:\n",
    "            os.environ[\"HF_USERNAME\"] = username\n",
    "        if token:\n",
    "            os.environ[\"HF_TOKEN\"] = token\n",
    "        if repo_name:\n",
    "            os.environ[\"HF_REPO_NAME\"] = repo_name\n",
    "\n",
    "        print(\"Loaded HuggingFace credentials from Colab secrets.\")\n",
    "    \n",
    "    except Exception:\n",
    "        # Not in Colab â†’ load from .env\n",
    "        load_dotenv()  # loads .env into os.environ if present\n",
    "        print(\"Loaded HuggingFace credentials from .env\")\n",
    "\n",
    "    # Final read\n",
    "    username = os.getenv(\"HF_USERNAME\")\n",
    "    repo_name = os.getenv(\"HF_REPO_NAME\", \"PEFT-optimization\")\n",
    "    token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "    # Validate\n",
    "    missing = []\n",
    "    if not username:\n",
    "        missing.append(\"HF_USERNAME\")\n",
    "    if not token:\n",
    "        missing.append(\"HF_TOKEN\")\n",
    "\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required environment variables: {missing}\")\n",
    "\n",
    "    return username, repo_name, token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f056863b",
   "metadata": {},
   "source": [
    "Download the model. The fourbit models download faster via unsloth, but we can change the model in `model_name`. Kept the rest of settings default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04d17ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n",
    "    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "\n",
    "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
    "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "\n",
    "    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbfddcf",
   "metadata": {},
   "source": [
    "Set up PEFT (LoRA). Settings default for now. Can tune later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec37a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cade3e7a",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3689b367",
   "metadata": {},
   "source": [
    "This section cleans up the training prompts from the FineTome dataset so that they match what llama expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9acee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136bc19c",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62158716",
   "metadata": {},
   "source": [
    "Create a trainer for the model. Using Hugginfaces default trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdfdfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = None, # Set to none to perform a full training run\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        #Below is where we save the weights as we go\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 100,\n",
    "        save_total_limit = 2, #Saving only the last 2 checkpoints\n",
    "        resume_from_checkpoint = True, #Resume from last checkpoint if possible\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dd0d59",
   "metadata": {},
   "source": [
    "Below we format the data so that we only train on the expected responses. The model doesn't benefit of training on what the user inputs, we just want to refine how it predicts the next word in the outputs, so we remove the user inputs from the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d547449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49988210",
   "metadata": {},
   "source": [
    "## Show GPU Memory Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25db0960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecdfa9c",
   "metadata": {},
   "source": [
    "Actually running the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277fdef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03bb2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ff7d67",
   "metadata": {},
   "source": [
    "# Saving Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4febbb3b",
   "metadata": {},
   "source": [
    "## Saving just LoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d53da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "\n",
    "#Save to HuggingFace Hub\n",
    "username, repo_name, token = load_hf_config()\n",
    "hf_repo_id = f\"{username}/{repo_name}\"\n",
    "\n",
    "print(f\"Pushing adapter to: {hf_repo_id}\")\n",
    "\n",
    "model.push_to_hub(hf_repo_id, token=token)\n",
    "tokenizer.push_to_hub(hf_repo_id, token=token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac09bf85",
   "metadata": {},
   "source": [
    "## Loading LoRA Adapter that we just saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c934fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabled for now. This was an example included in the template\n",
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe a tall tower in the capital of France.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2066ae",
   "metadata": {},
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "\n",
    "Using the `q4_k_m` conversion as this was recommended for speed and resource utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ec8d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save to q4_k_m GGUF\n",
    "model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "\n",
    "# Push to HuggingFace Hub GGUF\n",
    "username, repo_name, token = load_hf_config()\n",
    "model.push_to_hub_gguf(\n",
    "    f\"{username}/{repo_name}\",\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\",\n",
    "    token=token,\n",
    ")\n",
    "\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
