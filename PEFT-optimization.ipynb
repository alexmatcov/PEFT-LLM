{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d029f624",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2fe417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "project_dir = \"/content/drive/MyDrive/PEFT-optimization/\"\n",
    "os.makedirs(project_dir, exist_ok=True)\n",
    "os.chdir(project_dir)\n",
    "\n",
    "output_dir = os.path.join(project_dir, \"outputs\") #Stores checkpoints during training. Good if something crashes\n",
    "merged_dir = \"/content/merged_model\" #This is a large file that we don't need to save as we just use it for conversion to gguf\n",
    "gguf_dir = \"/content/gguf\" #Stores the converted GGUF model locally as we will upload to HF immediately after\n",
    "lora_dir = os.path.join(project_dir, \"lora_model\") #Stores the trained LoRA weights. Can use this to recreate merged model if needed\n",
    "\n",
    "# Persistent directories\n",
    "os.makedirs(lora_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Ephemeral directories\n",
    "os.makedirs(merged_dir, exist_ok=True)\n",
    "os.makedirs(gguf_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500d161d",
   "metadata": {},
   "source": [
    "Install unsloth on colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07e4692",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install python-dotenv\n",
    "!pip install -U \"transformers\" \"huggingface_hub\" # Install latest transformers and huggingface_hub to push GGUF. As recommended by Adrian\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git@nightly git+https://github.com/unslothai/unsloth-zoo.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ff4c6e",
   "metadata": {},
   "source": [
    "Get .env contents and keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d54ad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from google.colab import userdata\n",
    "\n",
    "def load_hf_config():\n",
    "    \"\"\"\n",
    "    Load Hugging Face credentials from:\n",
    "      1. Colab userdata secrets (if running on Colab)\n",
    "      2. Local .env file (if running locally)\n",
    "      3. Existing environment variables (fallback)\n",
    "    \"\"\"\n",
    "    # --- Check if running in Colab ---\n",
    "    try:\n",
    "        \n",
    "        # If this works, we are in Colab.\n",
    "        username = userdata.get(\"HF_USERNAME\")\n",
    "        token = userdata.get(\"HF_TOKEN\")\n",
    "        repo_name = userdata.get(\"HF_REPO_NAME\") or \"PEFT-optimization\"\n",
    "\n",
    "        if username:\n",
    "            os.environ[\"HF_USERNAME\"] = username\n",
    "        if token:\n",
    "            os.environ[\"HF_TOKEN\"] = token\n",
    "        if repo_name:\n",
    "            os.environ[\"HF_REPO_NAME\"] = repo_name\n",
    "\n",
    "        print(\"Loaded HuggingFace credentials from Colab secrets.\")\n",
    "    \n",
    "    except Exception:\n",
    "        # Not in Colab â†’ load from .env\n",
    "        load_dotenv()  # loads .env into os.environ if present\n",
    "        print(\"Loaded HuggingFace credentials from .env\")\n",
    "\n",
    "    # Final read\n",
    "    username = os.getenv(\"HF_USERNAME\")\n",
    "    repo_name = os.getenv(\"HF_REPO_NAME\", \"PEFT-optimization\")\n",
    "    token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "    # Validate\n",
    "    missing = []\n",
    "    if not username:\n",
    "        missing.append(\"HF_USERNAME\")\n",
    "    if not token:\n",
    "        missing.append(\"HF_TOKEN\")\n",
    "\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required environment variables: {missing}\")\n",
    "\n",
    "    return username, repo_name, token\n",
    "\n",
    "print(\"Testing credentials early...\")\n",
    "username, repo_name, token = load_hf_config()\n",
    "print(\"HF credentials OK, safe to continue!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f056863b",
   "metadata": {},
   "source": [
    "Download the model. The fourbit models download faster via unsloth, but we can change the model in `model_name`. Kept the rest of settings default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b19ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n",
    "    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "\n",
    "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
    "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "\n",
    "    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\", # Changed to a model from fourbit_models list. Smaller one also\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbfddcf",
   "metadata": {},
   "source": [
    "Set up PEFT (LoRA). Settings default for now. Can tune later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec37a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cade3e7a",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3689b367",
   "metadata": {},
   "source": [
    "This section cleans up the training prompts from the FineTome dataset so that they match what llama expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9acee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"mlabonne/FineTome-100k\", split = f\"train[:10000]\") #Reduced training size to 10k to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136bc19c",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62158716",
   "metadata": {},
   "source": [
    "Create a trainer for the model. Using Hugginfaces default trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdfdfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = True, # To make training faster, set this to True\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        #num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 600, #Changing to 600 for more in depth training (v2) (v1 used 50 steps)\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\", # Use \"adamw_torch\" when running locally, \"adamw_8bit\" works well on Colab\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = output_dir,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        #Below is where we save the weights as we go\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 100,\n",
    "        save_total_limit = 2, #Saving only the last 2 checkpoints\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dd0d59",
   "metadata": {},
   "source": [
    "Below we format the data so that we only train on the expected responses. The model doesn't benefit of training on what the user inputs, we just want to refine how it predicts the next word in the outputs, so we remove the user inputs from the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d547449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49988210",
   "metadata": {},
   "source": [
    "## Show GPU Memory Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25db0960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecdfa9c",
   "metadata": {},
   "source": [
    "Actually running the trainer. Resuming from checkpoint if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277fdef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "checkpoint = None\n",
    "if os.path.isdir(output_dir):\n",
    "    try:\n",
    "        checkpoint = get_last_checkpoint(output_dir)\n",
    "        if checkpoint is not None:\n",
    "            print(f\"Resuming training from checkpoint: {checkpoint}\")\n",
    "        else:\n",
    "            print(f\"No checkpoint found in '{output_dir}'. Starting from scratch.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get checkpoint from '{output_dir}': {e}\")\n",
    "        checkpoint = None\n",
    "\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03bb2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ff7d67",
   "metadata": {},
   "source": [
    "# Saving Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4febbb3b",
   "metadata": {},
   "source": [
    "## Saving just LoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d53da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(lora_dir) # Local saving\n",
    "tokenizer.save_pretrained(lora_dir)\n",
    "\n",
    "#Save to HuggingFace Hub\n",
    "username, repo_name, token = load_hf_config()\n",
    "hf_repo_id = f\"{username}/{repo_name}\"\n",
    "\n",
    "print(f\"Pushing adapter to: {hf_repo_id}\")\n",
    "\n",
    "model.push_to_hub(hf_repo_id, token=token)\n",
    "tokenizer.push_to_hub(hf_repo_id, token=token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac09bf85",
   "metadata": {},
   "source": [
    "## Loading LoRA Adapter that we just saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c934fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disabled for now. This was an example included in the template\n",
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Describe a tall tower in the capital of France.\"},\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    from transformers import TextStreamer\n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "    _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                    use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2066ae",
   "metadata": {},
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "\n",
    "Using llama.cpp for GGUF conversion instead of Unsloth as that ran out of memory on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b877faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA into a 16-bit HF model for llama.cpp conversion\n",
    "print(\"Merging LoRA into 16-bit base model...\")\n",
    "\n",
    "model.save_pretrained_merged(\n",
    "    merged_dir,\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    "    maximum_memory_usage=0.5,  # fraction of GPU memory; tweak if needed\n",
    ")\n",
    "\n",
    "print(f\"Merged model saved to: {merged_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed6216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Clone llama.cpp if not already there\n",
    "if not os.path.isdir(\"llama.cpp\"):\n",
    "    !git clone https://github.com/ggerganov/llama.cpp.git\n",
    "\n",
    "# Choose quantization\n",
    "quantization_method = \"q4_k_m\"  # good default for CPU inference\n",
    "gguf_filename = os.path.join(\n",
    "    gguf_dir,\n",
    "    f\"llama-3.2-1b-finetuned-{quantization_method}_v2.gguf\"\n",
    ")\n",
    "\n",
    "\n",
    "# Convert HF model in 'merged_model' to GGUF\n",
    "!python llama.cpp/convert_hf_to_gguf.py {merged_dir} \\\n",
    "    --outfile {gguf_filename} \\\n",
    "    --outtype {quantization_method}\n",
    "\n",
    "print(f\"GGUF file written to: {gguf_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dbb6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "username, repo_name, token = load_hf_config()\n",
    "gguf_repo_id = f\"{username}/{repo_name}\"\n",
    "\n",
    "api = HfApi()\n",
    "api.create_repo(repo_id=gguf_repo_id, exist_ok=True, token=token)\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=gguf_filename,\n",
    "    path_in_repo=os.path.basename(gguf_filename),\n",
    "    repo_id=gguf_repo_id,\n",
    "    token=token,\n",
    ")\n",
    "\n",
    "print(f\"Uploaded GGUF model to https://huggingface.co/{gguf_repo_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ec8d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if False: #Turned off for now\n",
    "    # Save to q4_k_m GGUF\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "\n",
    "    # Push to HuggingFace Hub GGUF\n",
    "    username, repo_name, token = load_hf_config()\n",
    "    model.push_to_hub_gguf(\n",
    "        f\"{username}/{repo_name}\",\n",
    "        tokenizer,\n",
    "        quantization_method=\"q4_k_m\",\n",
    "        token=token,\n",
    "    )\n",
    "\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
